# CPlusTorch

ğŸ”¥ A lightweight PyTorch implementation in C++ for educational and research purposes.

CPlusTorch is a from-scratch implementation of deep learning framework functionality similar to PyTorch, built entirely in C++. This project aims to provide a clear understanding of how modern deep learning frameworks work under the hood, making it an excellent educational resource for learning both C++ and deep learning concepts.

## âœ¨ Features

### Core Tensor Operations
- **Multi-dimensional Tensor Class**: Full support for n-dimensional arrays with dynamic shapes
- **Automatic Differentiation**: Complete autograd system for gradient computation
- **Broadcasting**: Automatic tensor broadcasting for element-wise operations
- **Memory Management**: Efficient memory handling with smart pointers

### Neural Network Components
- **Modular Architecture**: Base `Module` class for building custom neural networks
- **Linear Layers**: Fully connected layers with configurable input/output dimensions
- **Loss Functions**: Mean Squared Error (MSE) loss with gradient support
- **Parameter Management**: Automatic parameter registration and gradient tracking

### Optimization
- **Adam Optimizer**: Adaptive moment estimation with bias correction
- **Gradient Clipping**: Built-in gradient clipping to prevent gradient explosion
- **Parameter Updates**: Automatic parameter update mechanism

### Weight Initialization
- **Kaiming Uniform**: Proper weight initialization for deep networks
- **Bias Initialization**: Configurable bias initialization

## ğŸš€ Quick Start

### Prerequisites
- C++17 compatible compiler (GCC, Clang, MSVC)
- CMake 3.10 or higher

### Building the Project

```bash
# Clone the repository
git clone https://github.com/Y0sem1te/cplustorch.git
cd cplustorch

# Create build directory
mkdir build
cd build

# Configure and build
cmake ..
make  # or mingw32-make on Windows with MinGW
```

### Running the Demo

```bash
# Run the neural network training demo
./main
```

The demo trains a multi-layer perceptron to learn the function `y = sin(2Ï€x) + 0.5xÂ²`.

## ğŸ¬ Training Demo

Watch the neural network training process in action:

![Training Demo](assets/training_demo.gif)

*The demo shows real-time training progress with loss values and RÂ² scores as the model learns to approximate the target function.*

## ğŸ“ Project Structure

```
cplustorch/
â”œâ”€â”€ aten/                      # Core tensor library (similar to PyTorch's ATen)
â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â”œâ”€â”€ tensor.hpp         # Main tensor class
â”‚   â”‚   â”œâ”€â”€ tensor_iterator.hpp # Tensor iteration utilities
â”‚   â”‚   â””â”€â”€ native/
â”‚   â”‚       â””â”€â”€ ops.hpp        # Native operations
â”‚   â””â”€â”€ src/                   # Implementation files
â”œâ”€â”€ torch/                     # High-level neural network API
â”‚   â”œâ”€â”€ include/
â”‚   â”‚   â”œâ”€â”€ autograd/
â”‚   â”‚   â”‚   â””â”€â”€ engine.hpp     # Autograd engine
â”‚   â”‚   â”œâ”€â”€ nn/
â”‚   â”‚   â”‚   â”œâ”€â”€ module.hpp     # Base module class
â”‚   â”‚   â”‚   â”œâ”€â”€ init.hpp       # Weight initialization
â”‚   â”‚   â”‚   â””â”€â”€ modules/
â”‚   â”‚   â”‚       â”œâ”€â”€ linear.hpp # Linear layer
â”‚   â”‚   â”‚       â””â”€â”€ mse_loss.hpp # MSE loss function
â”‚   â”‚   â””â”€â”€ optim/
â”‚   â”‚       â”œâ”€â”€ optimizer.hpp  # Base optimizer
â”‚   â”‚       â””â”€â”€ adam.hpp       # Adam optimizer
â”‚   â””â”€â”€ src/                   # Implementation files
â”œâ”€â”€ main.cpp                   # Demo application
â”œâ”€â”€ CMakeLists.txt            # Build configuration
â””â”€â”€ README.md                 # This file
```

## ğŸ’¡ Usage Example

Here's a simple example of how to create and train a neural network:

```cpp
#include "./aten/include/tensor.hpp"
#include "./torch/include/nn/modules/linear.hpp"
#include "./torch/include/nn/modules/mse_loss.hpp"
#include "./torch/include/optim/adam.hpp"

// Define a simple MLP
class MLP : public torch::nn::Module {
public:
    MLP(size_t input_size, size_t hidden_size, size_t output_size) 
        : fc1(std::make_shared<torch::nn::Linear>(input_size, hidden_size)),
          fc2(std::make_shared<torch::nn::Linear>(hidden_size, output_size)) {
        register_module("fc1", fc1);
        register_module("fc2", fc2);
    }

    std::shared_ptr<at::Tensor> forward(const std::shared_ptr<at::Tensor> input) override {
        auto x = fc1->forward(input);
        // Apply ReLU activation
        for (size_t i = 0; i < x->data.size(); ++i) {
            if (x->data[i] < 0) x->data[i] = 0;
        }
        return fc2->forward(x);
    }

private:
    std::shared_ptr<torch::nn::Linear> fc1;
    std::shared_ptr<torch::nn::Linear> fc2;
};

int main() {
    // Create model
    MLP model(1, 16, 1);
    
    // Create optimizer
    auto params = model.parameters();
    torch::optim::Adam optimizer(params, 0.0001);
    
    // Create loss function
    torch::nn::MSELoss criterion;
    
    // Training loop
    for (int epoch = 0; epoch < 1000; ++epoch) {
        // Forward pass
        auto output = model.forward(input);
        auto loss = criterion.forward(output, target);
        
        // Backward pass
        optimizer.zero_grad();
        loss->grad[0] = 1.0;
        loss->backward();
        
        // Update parameters
        optimizer.step();
    }
    
    return 0;
}
```

## ğŸ¯ Current Capabilities

- âœ… Multi-dimensional tensor operations
- âœ… Automatic differentiation (autograd)
- âœ… Linear layers with bias
- âœ… MSE loss function
- âœ… Adam optimizer with gradient clipping
- âœ… Kaiming weight initialization
- âœ… Complete training pipeline
- âœ… Model parameter management

## ğŸš§ Roadmap

### Planned Features
- [ ] More activation functions (Sigmoid, Tanh, etc.)
- [ ] Additional loss functions (CrossEntropy, etc.)
- [ ] Convolutional layers
- [ ] Batch normalization
- [ ] Dropout regularization
- [ ] More optimizers (SGD, RMSprop)
- [ ] Learning rate scheduling
- [ ] Model serialization/deserialization
- [ ] GPU support (CUDA)

### Potential Enhancements
- [ ] SIMD optimizations
- [ ] Multi-threading support
- [ ] Memory pool allocation
- [ ] Advanced tensor operations
- [ ] Python bindings

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for bugs and feature requests.

### Development Guidelines
1. Follow C++17 standards
2. Maintain consistent code style
3. Add appropriate comments and documentation
4. Test your changes thoroughly

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by PyTorch framework architecture
- Built for educational purposes to understand deep learning internals
- Thanks to the open-source community for inspiration and best practices

## ğŸ“š Educational Value

This project is particularly valuable for:
- **Students** learning C++ and deep learning concepts
- **Researchers** who want to understand framework internals
- **Developers** interested in implementing ML frameworks
- **Anyone** curious about how PyTorch works under the hood

---
